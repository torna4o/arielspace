{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1d7fe926",
      "metadata": {
        "id": "1d7fe926"
      },
      "source": [
        "# Baseline Solution - Monte Carlo Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfcaafdf",
      "metadata": {
        "id": "cfcaafdf"
      },
      "source": [
        "## This notebook documents the baseline solution for ADC 2023. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f46c438b",
      "metadata": {
        "id": "f46c438b"
      },
      "source": [
        "## Overview\n",
        "Our challenge is to provide a conditional probability distribution for each target (7 in total) given an observation from the Ariel Space Telescope. \n",
        "\n",
        "Depending on the information content of the observation and the associated observation noise (which is a function of the instrument and the planetary system), the resultant error bounds on each target and their joint conditional distribution will be different.\n",
        "\n",
        "There are many directions you can take to tackle the problem on hand. We would like to get you started with our baseline solution. Inside this notebook you will find the setup for the baseline model, ways to compute the competition score and how to package the output into the competition format.\n",
        "\n",
        "Spectroscopic data alone are usually informative enough to provide a reasonable estiamte on the targets. After all, the trough and peaks in the spectra encoded information about the relative abundance of each gaseous species (see [Yip et al.](https://iopscience.iop.org/article/10.3847/1538-3881/ac1744>) ). The supplementary information also helps to better constrain some of the phyiscal quantities (see our discussion [here](https://www.ariel-datachallenge.space/ML/documentation/about) if you want to learn about the underlying physics :) , but I shall leave that to you. \n",
        "\n",
        "The baseline solution trains a CNN to output a deterministic estimate for each atmospheric target. At inference time, the network is made to produce probabilistic output by activating the dropout layers in the network (Monte Carlo Dropout, [Gal et al. 2016](https://arxiv.org/abs/1506.02142)). "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install taurex"
      ],
      "metadata": {
        "id": "6iVcttc7mvrz"
      },
      "id": "6iVcttc7mvrz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After installing Taurex, restart the runtime through clicking Runtime tab at the left top, and selecting \"Restart Runtime\"."
      ],
      "metadata": {
        "id": "-mCCRwfMadZK"
      },
      "id": "-mCCRwfMadZK"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # this will mount your Google Drive after your permission."
      ],
      "metadata": {
        "id": "vywqm8Ipm6k7"
      },
      "id": "vywqm8Ipm6k7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('####The folder where you put all helper functions and MCDropout files####')"
      ],
      "metadata": {
        "id": "7mq2Hy1InIPh"
      },
      "id": "7mq2Hy1InIPh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61537610",
      "metadata": {
        "id": "61537610"
      },
      "outputs": [],
      "source": [
        "import numpy as np # Numerical Python, for matrix operations and many other calculations\n",
        "import tensorflow as tf # Google's TensorFlow library, specific for Deep learning\n",
        "import pandas as pd # Creation of dataframes and facilitating data manipulation, similar to R dataframe\n",
        "from tensorflow import keras # Deep learning networks and other relevant parameters\n",
        "import h5py # to read and write HDF5 files\n",
        "import os # operating system package, navigating through folders, retrieving files etc.\n",
        "import matplotlib.pyplot as plt # main plotting library, similar to MATLAB plotting\n",
        "from tqdm import tqdm # Progress monitoring bars in model creation and training/validation/test steps\n",
        "from helper import *\n",
        "import taurex # atmospheric retrieval framework, bayesian inverse, \n",
        "from submit_format import to_competition_format\n",
        "from posterior_utils import *\n",
        "from spectral_metric import *\n",
        "from FM_utils_final import *\n",
        "from taurex import log\n",
        "taurex.log.disableLogging()\n",
        "from MCDropout import MC_Convtrainer # baseline CNN network\n",
        "from preprocessing import *"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10834d90",
      "metadata": {
        "id": "10834d90"
      },
      "source": [
        "### Fix seed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ace7f9f",
      "metadata": {
        "id": "0ace7f9f"
      },
      "outputs": [],
      "source": [
        "SEED=42"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b708058f",
      "metadata": {
        "id": "b708058f"
      },
      "source": [
        "### Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5cb082a",
      "metadata": {
        "id": "c5cb082a"
      },
      "outputs": [],
      "source": [
        "RJUP = 69911000\n",
        "MJUP = 1.898e27\n",
        "RSOL = 696340000"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieve TauRex folders"
      ],
      "metadata": {
        "id": "ljqio2Wmxuhb"
      },
      "id": "ljqio2Wmxuhb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieve HITRAN files"
      ],
      "metadata": {
        "id": "0PM-2TAbQuPr"
      },
      "id": "0PM-2TAbQuPr"
    },
    {
      "cell_type": "code",
      "source": [
        "# CIA collision induced absorption database, and cross section (xsec) files\n",
        "# You should save these to specific files for CIA and XSEC, to later use them for forward modeling\n",
        "\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "urlretrieve('https://www.dropbox.com/sh/1njwmcqvv8zj3sy/AAA9ZSWfyFGy0y9xoVDlk2-xa/cia/HITRAN/H2-H2.db?dl=1', '####folder for CIA####/H2-H2.db') # modify the second part\n",
        "urlretrieve('https://www.dropbox.com/sh/1njwmcqvv8zj3sy/AAAwLZaNnzZpRnSyyU-ctdIxa/cia/HITRAN/H2-He.db?dl=1', '####folder for CIA####/H2-He.db') # modify the second part\n",
        "\n",
        "urlretrieve('https://www.dropbox.com/sh/1njwmcqvv8zj3sy/AADYnkfP_38GjVQ5iZP_pCXva/xsec/1H2-16O__POKAZATEL.R15000_0.3-50mu.xsec.TauREx.h5?dl=1', '####folder for XSEC####/1H2-16O__POKAZATEL.R15000_0.3-50mu.xsec.TauREx.h5') # modify the second part\n",
        "urlretrieve('https://www.dropbox.com/sh/1njwmcqvv8zj3sy/AABJg05TMEUiy-EMcrJs3qdua/xsec/12C-1H4__YT34to10.R15000_0.3-50mu.xsec.TauREx.h5?dl=1', '####folder for XSEC####/12C-1H4__YT34to10.R15000_0.3-50mu.xsec.TauREx.h5') # modify the second part\n",
        "urlretrieve('https://www.dropbox.com/sh/1njwmcqvv8zj3sy/AACLAVUeYkESGEVoF93lWvZya/xsec/12C-16O__Li2015.R15000_0.3-50mu.xsec.TauREx.h5?dl=1', '####folder for XSEC####/12C-16O__Li2015.R15000_0.3-50mu.xsec.TauREx.h5') # modify the second part\n",
        "urlretrieve('https://www.dropbox.com/sh/1njwmcqvv8zj3sy/AABlnQoo7JE4tjABR2PKiy6Ia/xsec/12C-16O2__HITEMP.R15000_0.3-50mu.xsec.TauREx.h5?dl=1', '####folder for XSEC####/12C-16O2__HITEMP.R15000_0.3-50mu.xsec.TauREx.h5?dl=1') # modify the second part\n",
        "urlretrieve('https://www.dropbox.com/sh/1njwmcqvv8zj3sy/AADCB0UTVz09Q7VcIy62KmXXa/xsec/14N-1H3__CoYuTe.R15000_0.3-50mu.xsec.TauREx.h5?dl=1', '####folder for XSEC####/14N-1H3__CoYuTe.R15000_0.3-50mu.xsec.TauREx.h5?dl=1') # modify the second part"
      ],
      "metadata": {
        "id": "Ec6hPZB_Qs-z"
      },
      "id": "Ec6hPZB_Qs-z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Retrieve training data**"
      ],
      "metadata": {
        "id": "Cc0cIAtUpb1L"
      },
      "id": "Cc0cIAtUpb1L"
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve\n",
        "\n",
        "urlretrieve('https://www.ariel-datachallenge.space/static/data/FullDataset.zip', '####The folder where you put all helper functions and MCDropout files####/FullDataset.zip') # modify the second part\n"
      ],
      "metadata": {
        "id": "8GJeL0SDpbTs"
      },
      "id": "8GJeL0SDpbTs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting the content of Training & Test Data"
      ],
      "metadata": {
        "id": "CpVymZxisPNc"
      },
      "id": "CpVymZxisPNc"
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "with ZipFile('/content/drive/MyDrive/ari/FullDataset.zip', 'r') as zippy:\n",
        "    zippy.extractall(path=os.getcwd())"
      ],
      "metadata": {
        "id": "lSCguQuvq9yT"
      },
      "id": "lSCguQuvq9yT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e21a8402",
      "metadata": {
        "id": "e21a8402"
      },
      "source": [
        "## Read training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6de11f8",
      "metadata": {
        "id": "d6de11f8"
      },
      "outputs": [],
      "source": [
        "training_path = '####The folder where you put all helper functions and MCDropout files####/TrainingData' # modify\n",
        "test_path = '####The folder where you put all helper functions and MCDropout files####/TestData' # modify\n",
        "training_GT_path = os.path.join(training_path, 'Ground Truth Package')\n",
        "\n",
        "spectral_training_data = h5py.File(os.path.join(training_path,'SpectralData.hdf5'),\"r\")\n",
        "aux_training_data = pd.read_csv(os.path.join(training_path,'AuxillaryTable.csv'))\n",
        "soft_label_data = pd.read_csv(os.path.join(training_GT_path, 'FM_Parameter_Table.csv'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b9b068d",
      "metadata": {
        "id": "3b9b068d"
      },
      "source": [
        "## Extract Spectral data\n",
        "Spectral data lives in a h5py format, which is useful for navigating different cases, but their format makes it difficult to bulk manage them. The helper function helps to transform the h5py file into a matrix of size N x 52 x 4\n",
        "where N is the number of training examples, 52 is the number of wavelength channels and 4 is the observation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41e90f7e",
      "metadata": {
        "id": "41e90f7e"
      },
      "outputs": [],
      "source": [
        "spec_matrix = to_observed_matrix(spectral_training_data,aux_training_data)\n",
        "print(\"spectral matrix shape:\", spec_matrix.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d708b807",
      "metadata": {
        "id": "d708b807"
      },
      "source": [
        "# Visualising a single spectrum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8b82614",
      "metadata": {
        "id": "a8b82614"
      },
      "outputs": [],
      "source": [
        "def visualise_spectrum(spectrum):\n",
        "    fig = plt.figure(figsize=(10,6))\n",
        "    ## multiple by 100 to turn it into percentage. \n",
        "    plt.errorbar(x=spectrum[:,0], y= spectrum[:,1]*100, yerr=spectrum[:,2]*100 )\n",
        "    ## we tend to visualise it in log-scale\n",
        "    plt.xscale('log')\n",
        "    plt.xlabel('Wavelength (micron)')\n",
        "    plt.ylabel('Transit depth (%)')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5df6f122",
      "metadata": {
        "id": "5df6f122"
      },
      "outputs": [],
      "source": [
        "visualise_spectrum(spec_matrix[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9e3c4c0",
      "metadata": {
        "id": "e9e3c4c0"
      },
      "outputs": [],
      "source": [
        "## lets look at another one\n",
        "visualise_spectrum(spec_matrix[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a50d49f8",
      "metadata": {
        "id": "a50d49f8"
      },
      "source": [
        "it is immediately apparent that the average transit depth between two spectra can change for over an order of magnitude. The magnitude of the uncertainty can also change accordingly ( and is a function of the planetary system, brightness of the host star and instrument response function). "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6386b02d",
      "metadata": {
        "id": "6386b02d"
      },
      "source": [
        "## Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a68c8aa3",
      "metadata": {
        "id": "a68c8aa3"
      },
      "source": [
        "### Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9946dccd",
      "metadata": {
        "id": "9946dccd"
      },
      "outputs": [],
      "source": [
        "repeat = 5\n",
        "threshold = 0.8 ## for train valid split.\n",
        "N = 5000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c49ae719",
      "metadata": {
        "id": "c49ae719"
      },
      "source": [
        "We can safely discard wlgrid (wavelength grid) and wlwidth (width of wavelength) since they are unchanged in the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "510cff82",
      "metadata": {
        "id": "510cff82"
      },
      "source": [
        "### Extract Spectrum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a99e956a",
      "metadata": {
        "id": "a99e956a"
      },
      "outputs": [],
      "source": [
        "## extract the noise\n",
        "noise = spec_matrix[:N,:,2]\n",
        "## We will incorporate the noise profile into the observed spectrum by treating the noise as Gaussian noise.\n",
        "spectra = spec_matrix[:N,:,1]\n",
        "wl_channels = len(spec_matrix[0,:,0])\n",
        "global_mean = np.mean(spectra)\n",
        "global_std = np.std(spectra)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d9e0795",
      "metadata": {
        "id": "8d9e0795"
      },
      "source": [
        "### Adding an additional feature - radius of the star \n",
        "Most of the time we know something about the planetary system before we even attempt to make an observation (we cant just point randomly with a multi-million euros instrument!). Some of these auxillary data may be useful for retrieval, here we are only using the radius of the star."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "890852c7",
      "metadata": {
        "id": "890852c7"
      },
      "outputs": [],
      "source": [
        "## add Rstar \n",
        "Rs = aux_training_data[['star_radius_m',]]\n",
        "## we would prefer to use Rsol\n",
        "Rs['star_radius'] = Rs['star_radius_m']/RSOL\n",
        "Rs = Rs.drop(['star_radius_m'],axis=1)\n",
        "Rs = Rs.iloc[:N, :]\n",
        "mean_Rs = Rs.mean()\n",
        "stdev_Rs = Rs.std()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "520b5ac2",
      "metadata": {
        "id": "520b5ac2"
      },
      "source": [
        "### Get targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "749830e8",
      "metadata": {
        "id": "749830e8"
      },
      "outputs": [],
      "source": [
        "target_labels = ['planet_radius','planet_temp','log_H2O','log_CO2','log_CO','log_CH4','log_NH3']\n",
        "targets = soft_label_data.iloc[:N][target_labels]\n",
        "num_targets = targets.shape[1]\n",
        "targets_mean = targets.mean()\n",
        "targets_std = targets.std()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91fe76dd",
      "metadata": {
        "id": "91fe76dd"
      },
      "source": [
        "## Train/valid Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6e265d6",
      "metadata": {
        "id": "c6e265d6"
      },
      "outputs": [],
      "source": [
        "ind = np.random.rand(len(spectra)) < threshold\n",
        "training_spectra, training_Rs,training_targets, training_noise = spectra[ind],Rs[ind],targets[ind], noise[ind]\n",
        "valid_spectra, valid_Rs, valid_targets = spectra[~ind],Rs[~ind],targets[~ind]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e5a9501",
      "metadata": {
        "id": "5e5a9501"
      },
      "source": [
        "## Augment the dataset with noise (create multiple instances)\n",
        "Observational noise from Ariel forms an important part of the challenge, any model must recognise that the observation are not absolute measurement and could vary (according to the uncertainty), as that will affect the uncertainty associated with our atmospheric targets. Here we try to incorporate these information by augmenting the data with the mean noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0393e56d",
      "metadata": {
        "id": "0393e56d"
      },
      "outputs": [],
      "source": [
        "aug_spectra = augment_data_with_noise(training_spectra, training_noise, repeat)\n",
        "aug_Rs = np.tile(training_Rs.values,(repeat,1))\n",
        "aug_targets = np.tile(training_targets.values,(repeat,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ee4036a",
      "metadata": {
        "id": "2ee4036a"
      },
      "source": [
        "### Standardise the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9787437e",
      "metadata": {
        "id": "9787437e"
      },
      "source": [
        "### spectra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fcb9ec3",
      "metadata": {
        "id": "1fcb9ec3"
      },
      "outputs": [],
      "source": [
        "## standardise the input using global mean and stdev\n",
        "std_aug_spectra = standardise(aug_spectra, global_mean, global_std)\n",
        "std_aug_spectra = std_aug_spectra.reshape(-1, wl_channels)\n",
        "std_valid_spectra = standardise(valid_spectra, global_mean, global_std)\n",
        "std_valid_spectra = std_valid_spectra.reshape(-1, wl_channels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76378a24",
      "metadata": {
        "id": "76378a24"
      },
      "source": [
        "### radius"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00714009",
      "metadata": {
        "id": "00714009"
      },
      "outputs": [],
      "source": [
        "## standardise\n",
        "std_aug_Rs= standardise(aug_Rs, mean_Rs.values.reshape(1,-1), stdev_Rs.values.reshape(1,-1))\n",
        "std_valid_Rs= standardise(valid_Rs, mean_Rs, stdev_Rs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9f848b1",
      "metadata": {
        "id": "f9f848b1"
      },
      "source": [
        "### target\n",
        "We are asking the model to provide estimates for 6 atmospheric targets. In this example will be performing a supervised learning task. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39e863c8",
      "metadata": {
        "id": "39e863c8"
      },
      "outputs": [],
      "source": [
        "std_aug_targets = standardise(aug_targets, targets_mean.values.reshape(1,-1), targets_std.values.reshape(1,-1))\n",
        "std_valid_targets = standardise(valid_targets, targets_mean, targets_std)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ed42bc8",
      "metadata": {
        "id": "2ed42bc8"
      },
      "source": [
        "# Setup network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac6e99c1",
      "metadata": {
        "id": "ac6e99c1"
      },
      "source": [
        "### hyperparameter settings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4212023a",
      "metadata": {
        "id": "4212023a"
      },
      "outputs": [],
      "source": [
        "batch_size= 32\n",
        "lr= 1e-3\n",
        "epochs = 30\n",
        "filters = [32,64,64]\n",
        "dropout = 0.1\n",
        "# number of examples to generate in evaluation time (5000 is max for this competition)\n",
        "N_samples = 5000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8dc9c85",
      "metadata": {
        "id": "b8dc9c85"
      },
      "source": [
        "We followed [Yip et al.](https://iopscience.iop.org/article/10.3847/1538-3881/ac1744>) and adopted a simple CNN structure and loss function. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "515d9a79",
      "metadata": {
        "id": "515d9a79"
      },
      "outputs": [],
      "source": [
        "model = MC_Convtrainer(wl_channels,num_targets,dropout,filters)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d9fea61",
      "metadata": {
        "id": "1d9fea61"
      },
      "source": [
        "### Compile model and Train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c17ccab9",
      "metadata": {
        "id": "c17ccab9"
      },
      "outputs": [],
      "source": [
        "## compile model and run\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(lr),\n",
        "    loss='mse',)\n",
        "model.fit([std_aug_spectra,std_aug_Rs], \n",
        "          std_aug_targets, \n",
        "          validation_data=([std_valid_spectra, std_valid_Rs],std_valid_targets),\n",
        "          batch_size=batch_size, \n",
        "          epochs=epochs, \n",
        "          shuffle=False,)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional saving for model to a file, or loading a model from a file"
      ],
      "metadata": {
        "id": "2t3qAWV2f065"
      },
      "id": "2t3qAWV2f065"
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving a model for later use\n",
        "model.save(\"model.h5\") # you can save a network to a file for being able to use it later\n",
        "model = keras.models.load_model(\"model.h5\") # This will load the previously saved model"
      ],
      "metadata": {
        "id": "zbHd4CCMfYI8"
      },
      "id": "zbHd4CCMfYI8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evalute model with validation data"
      ],
      "metadata": {
        "id": "1tBLdhW-f-0J"
      },
      "id": "1tBLdhW-f-0J"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cde5fbcc",
      "metadata": {
        "id": "cde5fbcc"
      },
      "outputs": [],
      "source": [
        "## select the corresponding GT for the validation data, and in the correct order.\n",
        "index= np.arange(len(ind))\n",
        "valid_index = index[~ind]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d91f79a9",
      "metadata": {
        "id": "d91f79a9"
      },
      "outputs": [],
      "source": [
        "instances = N_samples\n",
        "y_valid_distribution = np.zeros((instances, len(std_valid_spectra), num_targets ))\n",
        "for i in tqdm(range(instances)):\n",
        "    \n",
        "    y_pred_valid = model([std_valid_spectra,std_valid_Rs],training=True)\n",
        "    y_valid_distribution[i] += y_pred_valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3a14bd4",
      "metadata": {
        "id": "e3a14bd4"
      },
      "outputs": [],
      "source": [
        "y_valid_distribution = y_valid_distribution.reshape(-1,num_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7adf5237",
      "metadata": {
        "id": "7adf5237"
      },
      "outputs": [],
      "source": [
        "y_pred_valid_org = transform_and_reshape(y_valid_distribution,targets_mean, targets_std,instances,N_testdata=len(std_valid_spectra))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d144580",
      "metadata": {
        "id": "5d144580"
      },
      "outputs": [],
      "source": [
        "tr1 = y_pred_valid_org\n",
        "# weight takes into account the importance of each point in the tracedata. for now we just assume them to be equally weighted\n",
        "weights1 = np.ones((tr1.shape[0],tr1.shape[1]))/np.sum(np.ones(tr1.shape[1]) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d46984b3",
      "metadata": {
        "id": "d46984b3"
      },
      "outputs": [],
      "source": [
        "trace_GT = h5py.File(os.path.join(training_GT_path, 'Tracedata.hdf5'),\"r\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86591747",
      "metadata": {
        "id": "86591747"
      },
      "source": [
        "## posterior scores \n",
        "This score accounts for 80% of the final score and it is based on the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f80c06d8",
      "metadata": {
        "id": "f80c06d8"
      },
      "outputs": [],
      "source": [
        "posterior_scores = []\n",
        "bounds_matrix = default_prior_bounds()\n",
        "for idx, pl_idx in enumerate(valid_index):\n",
        "    tr_GT = trace_GT[f'Planet_train{pl_idx+1}']['tracedata'][()]\n",
        "    weights_GT = trace_GT[f'Planet_train{pl_idx+1}']['weights'][()]\n",
        "    ## there are cases without ground truth, we will skip over them for this baseline\n",
        "    ## but every example in leaderboard and final evaluation set will have a complementary ground truth\n",
        "    if np.isnan(tr_GT).sum() == 1:\n",
        "        continue\n",
        "    # compute posterior loss\n",
        "    score = compute_posterior_loss(tr1[idx], weights1[idx], tr_GT, weights_GT, bounds_matrix)\n",
        "    posterior_scores.append(score)\n",
        "avg_posterior_score = np.mean(posterior_scores)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37d80018",
      "metadata": {
        "id": "37d80018"
      },
      "outputs": [],
      "source": [
        "print(avg_posterior_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e5cbee0",
      "metadata": {
        "id": "6e5cbee0"
      },
      "source": [
        "## spectral scores \n",
        "This score accounts for 20% of the final score and it is based on a pre-selected, classified subset of the entire dataset.\n",
        "It takes a while to compute the score, even for 100 samples, so we will randomly draw 20 in this case to illustrate the idea.\n",
        "\n",
        "**CAUTION: To use this metric you must have taurex and their linelists available on your local environment. Please refer to README.MD on the github repo for more information**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bd09e96",
      "metadata": {
        "id": "4bd09e96"
      },
      "outputs": [],
      "source": [
        "N_examples = 20 ## number of test examples to go through\n",
        "N_samples = 10 ## number of quantiles to sample (fixed to 10 in the competition)\n",
        "q_list = np.linspace(0.01,0.99,N_samples)\n",
        "## beta - weight of the posterior loss [0,1], and the weight of spectral loss will decrease accordingly. \n",
        "beta = 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78a7590e",
      "metadata": {
        "id": "78a7590e"
      },
      "outputs": [],
      "source": [
        "## Path variables\n",
        "opacity_path=\"####folder for XSEC####\"\n",
        "CIA_path=\"####folder for CIA####\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "711ce7bc",
      "metadata": {
        "id": "711ce7bc"
      },
      "outputs": [],
      "source": [
        "## read in spectral grid\n",
        "ariel_wlgrid, ariel_wlwidth, ariel_wngrid, ariel_wnwidth = ariel_resolution()\n",
        "## Initialise base T3 model for ADC2023\n",
        "fm = initialise_forward_model(opacity_path, CIA_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d06bd7a",
      "metadata": {
        "id": "5d06bd7a"
      },
      "outputs": [],
      "source": [
        "# raed auxillary information from the input file (Provided from ADC2023)\n",
        "aux_df = aux_training_data\n",
        "# ensure the dimensionality matches forward model's input.\n",
        "Rs = aux_df['star_radius_m']/RSOL\n",
        "# Rp = aux_df['planet_radius_m']/RJUP\n",
        "Mp = aux_df['planet_mass_kg']/MJUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cea7ec6f",
      "metadata": {
        "id": "cea7ec6f"
      },
      "outputs": [],
      "source": [
        "## select few random validation data for spectral loss computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "657e7a27",
      "metadata": {
        "id": "657e7a27"
      },
      "outputs": [],
      "source": [
        "spectral_scores = []\n",
        "bounds_matrix = default_prior_bounds()\n",
        "for idx, pl_idx in enumerate(valid_index):\n",
        "    ## put an early stop here as it will take forever to go through 5000 examples. \n",
        "    if idx == 20:\n",
        "        break\n",
        "    tr_GT = trace_GT[f'Planet_train{pl_idx+1}']['tracedata'][()]\n",
        "    weights_GT = trace_GT[f'Planet_train{pl_idx+1}']['weights'][()]\n",
        "    # again to avoid unlabelled data\n",
        "    if np.isnan(tr_GT).sum() == 1:\n",
        "        continue\n",
        "\n",
        "    proxy_compute_spectrum = setup_dedicated_fm(fm, idx, Rs, Mp, ariel_wngrid, ariel_wnwidth )\n",
        "\n",
        "    score = compute_spectral_loss(tr1[idx], weights1[idx], tr_GT,weights_GT,bounds_matrix,proxy_compute_spectrum,q_list)\n",
        "    spectral_scores.append(score)\n",
        "avg_spectral_score = np.mean(spectral_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ff22765",
      "metadata": {
        "id": "8ff22765"
      },
      "outputs": [],
      "source": [
        "final_score = (1-beta)*avg_spectral_score + beta *avg_posterior_score\n",
        "print(f\"final loss is {final_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c4423bb",
      "metadata": {
        "id": "9c4423bb"
      },
      "source": [
        "# Generate prediction for leaderboard"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40af6336",
      "metadata": {
        "id": "40af6336"
      },
      "source": [
        "### load leaderboard data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8488fe6b",
      "metadata": {
        "id": "8488fe6b"
      },
      "outputs": [],
      "source": [
        "spec_test_data = h5py.File(os.path.join(test_path,'SpectralData.hdf5'),\"r\")\n",
        "aux_test_data = pd.read_csv(os.path.join(test_path,'AuxillaryTable.csv'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4ce08e4",
      "metadata": {
        "id": "c4ce08e4"
      },
      "source": [
        "### same pre-processing as before..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "725d88ef",
      "metadata": {
        "id": "725d88ef"
      },
      "outputs": [],
      "source": [
        "test_spec_matrix = to_observed_matrix(spec_test_data,aux_test_data )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e932a24",
      "metadata": {
        "id": "1e932a24"
      },
      "outputs": [],
      "source": [
        "std_test_spectra = standardise(test_spec_matrix[:,:,1], global_mean, global_std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c4ee734",
      "metadata": {
        "id": "5c4ee734"
      },
      "outputs": [],
      "source": [
        "test_Rs = aux_test_data[['star_radius_m']]\n",
        "## we would prefer to use RSol \n",
        "test_Rs['star_radius'] = test_Rs['star_radius_m']/RSOL\n",
        "test_Rs = test_Rs.drop(['star_radius_m'],axis=1)\n",
        "std_test_Rs= standardise(test_Rs, mean_Rs, stdev_Rs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47661bea",
      "metadata": {
        "id": "47661bea"
      },
      "source": [
        "## Predict and postprocess\n",
        "We will sample 5000 times by activating dropout at inference phase. This is done explicitly via training = True. Note that in the competition, any sample size bigger than 5000 will NOT be accepted. However, the sample size must have a minimum of 1000 points to be a valid submission. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b85c56d",
      "metadata": {
        "id": "0b85c56d"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.set_random_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e55168d",
      "metadata": {
        "id": "9e55168d"
      },
      "outputs": [],
      "source": [
        "instances = N_samples+ 4990 # for making 5000 instances, the challenge requires instances between 1000 and 5000\n",
        "y_pred_distribution = np.zeros((instances, len(std_test_spectra), num_targets ))\n",
        "for i in tqdm(range(instances)):\n",
        "    \n",
        "    y_pred = model([std_test_spectra,std_test_Rs],training=True)\n",
        "    y_pred_distribution[i] += y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3cbf4e3",
      "metadata": {
        "id": "c3cbf4e3"
      },
      "outputs": [],
      "source": [
        "y_pred_distribution = y_pred_distribution.reshape(-1,num_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be48c5e3",
      "metadata": {
        "id": "be48c5e3"
      },
      "outputs": [],
      "source": [
        "#y_pred_test_org = transform_and_reshape(y_pred_distribution,targets_mean, targets_std,instances,N_testdata=len(std_test_spectra))\n",
        "y_pred_test_org = transform_and_reshape(y_pred_distribution,targets_mean, targets_std,instances,N_testdata=len(std_test_spectra))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c03019da",
      "metadata": {
        "id": "c03019da"
      },
      "source": [
        "## Package output into desired format\n",
        "We follow specific formats in the competition, to help make the process as painless as possible, we have included a few helper functions to make sure you have the right format in place for the submission. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tracedata = y_pred_test_org\n",
        "# weight takes into account the importance of each point in the tracedata. \n",
        "weight = np.ones((tracedata.shape[0],tracedata.shape[1]))/np.sum(np.ones(tracedata.shape[1]) )"
      ],
      "metadata": {
        "id": "okfYRzgLGMrL"
      },
      "id": "okfYRzgLGMrL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0a4aa48",
      "metadata": {
        "id": "a0a4aa48"
      },
      "outputs": [],
      "source": [
        "submission = to_competition_format(tracedata, \n",
        "                                        weight, \n",
        "                                        name=\"submission.hdf5\") # you may modify this for different submissions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca0d9a33",
      "metadata": {
        "id": "ca0d9a33"
      },
      "source": [
        "## check!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec52ac61",
      "metadata": {
        "id": "ec52ac61"
      },
      "source": [
        "## Future work"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6808d3c3",
      "metadata": {
        "id": "6808d3c3"
      },
      "source": [
        "There are different direction to take from here on, let us summarise the shortcomings of this model:\n",
        "- The data preprocessing is quite simplistic and could have invested with more efforts.\n",
        "- we have only used 5000 data points, instead of the full dataset\n",
        "- we didnt train the model with results from the retrieval ( Tracedata.hdf5), which are the GT for this competition.\n",
        "- The conditional distribution from MCDropout is very restricted and Gaussian-like\n",
        "- So far we havent considered the atmospheric targets as a joint distribution\n",
        "- We have only used stellar radius from the auxillary information\n",
        "- We have not done any hyperparameter tuning \n",
        "- the train test split here is not clean, as in, we split the data after we have augmented the data, which results in information leakage to the validation data. There is no leakage to the test data though."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}